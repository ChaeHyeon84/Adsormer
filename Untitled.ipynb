{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a916eb-7396-410d-91a4-662a6c66c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ase.io import read,write\n",
    "from ase import Atoms, Atom\n",
    "import numpy as np\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "import pymatgen as mg\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchtext import data # torchtext.data 임포트\n",
    "from torchtext.data import Iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# from data import CIFData, AtomCustomJSONInitializer, GaussianDistance\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "def remove_adsorbates(atoms):\n",
    "    binding_sites = []\n",
    "    adsorbates_index = []\n",
    "    \n",
    "    for atom in atoms:\n",
    "        if atom.tag == 1:\n",
    "            binding_sites.append([atom.symbol,list(atom.position)])\n",
    "            adsorbates_index.append(atom.index)\n",
    "    del atoms[adsorbates_index]\n",
    "    bare_slab = atoms.copy()\n",
    "    \n",
    "    return binding_sites, bare_slab\n",
    "\n",
    "def get_nearest_atoms(atoms):\n",
    "    # view(atoms)\n",
    "    binding_sites, slab = remove_adsorbates(atoms)\n",
    "    binding_sites.sort(key = lambda x: x[1][2] )    \n",
    "    \n",
    "    copied_atom = slab.copy()\n",
    "    copied_atom = copied_atom.repeat((2,2,1))\n",
    "    copied_atom += Atom(binding_sites[0][0],binding_sites[0][1],tag =1 )\n",
    "    ads_index = np.where((copied_atom.get_tags()) ==1)[0][0]\n",
    "    \n",
    "    structure = AseAtomsAdaptor.get_structure(copied_atom)\n",
    "    # nn = structure.get_neighbors(site=structure[ads_index] , r= min(structure.lattice.abc))\n",
    "    nn = structure.get_neighbors(site=structure[ads_index] , r= 10)\n",
    "    nn.sort(key = lambda x : x[1]) # sort nearest atoms\n",
    "    nn_index = [nn[i][2] for i in range(len(nn))]\n",
    "    \n",
    "    \n",
    "    # view(copied_atom)\n",
    "    return copied_atom, nn_index\n",
    "\n",
    "def get_atom_property(atoms):\n",
    "    global feature\n",
    "    feature_df = pd.DataFrame(columns=[f'feat{i}' for i in range(10)])\n",
    "    atom, nn_index = get_nearest_atoms(atoms)\n",
    "    elements = [atom[i].symbol for i in nn_index[:20]]\n",
    "    def block_to_num(block):\n",
    "        \"\"\"\n",
    "        Convert blokc to number\n",
    "\n",
    "        Args:\n",
    "            block (str) : 's', 'p', 'd' or 'f'\n",
    "\n",
    "        Return:\n",
    "            int : 1, 2, 3, 4\n",
    "        \"\"\"\n",
    "\n",
    "        if block == 's':\n",
    "            return 1\n",
    "        elif block == 'p':\n",
    "            return 2\n",
    "        elif block == 'd':\n",
    "            return 3\n",
    "        elif block == 'f':\n",
    "            return 4\n",
    "\n",
    "    for el in elements:\n",
    "        e = mg.core.Element(el)\n",
    "        atomic_number = e.Z\n",
    "        average_ionic_radius = e.average_ionic_radius.real\n",
    "\n",
    "        # Lowest oxidiation state of the element is used as common oxidation state\n",
    "        common_oxidation_states = e.common_oxidation_states[0]\n",
    "        Pauling_electronegativity = e.X\n",
    "        row = e.row\n",
    "        group = e.group\n",
    "        thermal_conductivity = e.thermal_conductivity.real\n",
    "        boiling_point = e.boiling_point.real\n",
    "        melting_point = e.melting_point.real\n",
    "        block = block_to_num(e.block)\n",
    "        IE = e.ionization_energy\n",
    "        \n",
    "        feature_df.loc[len(feature_df)] = [atomic_number, common_oxidation_states, Pauling_electronegativity, \n",
    "                                           row, group, thermal_conductivity, boiling_point,\n",
    "                                           melting_point, block, IE]\n",
    "        feature = feature_df.to_numpy()\n",
    "        feature = torch.Tensor(feature).long()\n",
    "        # feature.unsqueeze_(0).shape\n",
    "    return feature\n",
    "\n",
    "\n",
    "def train(train_loader, model,global_step, criterion, optimizer, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        # mae_erros = []\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, targets = data\n",
    "        a,b = inputs.shape, targets.shape\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs,targets)\n",
    "        outputs =  outputs.type(torch.float32)\n",
    "        loss = criterion(outputs, targets.to(torch.float32))\n",
    "        # loss.requires_grad_(True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        print(f'[{epoch +1}, {i+1:5d}] loss: {loss:.4f}')\n",
    "        mae_error = torch.mean(abs(outputs-targets))\n",
    "        mae_errors_all.append(mae_error)\n",
    "        print(f'mae error: {mae_error} ')\n",
    "        if i % 10 == 9:\n",
    "            # print(f'[{epoch +1}, {i+1:5d}] loss: {running_loss/ 2000:.3f}')\n",
    "            # print(f'mae error: {mae_error} ')\n",
    "            running_loss = 0.0\n",
    "        global_step += 1\n",
    "    return global_step\n",
    "\n",
    "def validation(val_loader , model, global_step,criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cnt = 0\n",
    "    for i, data in enumerate(val_loader,0):\n",
    "        # mae_erros = []\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, targets = data\n",
    "        a,b = inputs.shape, targets.shape\n",
    "        pred = model(inputs, targets)\n",
    "        pred =  pred.type(torch.float32)\n",
    "        loss = criterion(pred, targets.to(torch.float32))\n",
    "        total_loss += loss.item() * len(i)\n",
    "        total_cnt += len(batch)\n",
    "    val_loss = total_loss / total_cnt\n",
    "    print(\"Validation Loss\", val_loss)\n",
    "        \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "class make_dataset(Dataset):\n",
    "    def __init__(self, root_dir, dmin = 0, step = 0.2, random_seed = 123):\n",
    "        self.root_dir = root_dir\n",
    "        target_file = os.path.join(self.root_dir, 'data/co_target.csv')\n",
    "        target_df = pd.read_csv(f'{self.root_dir}data/co_target.csv')\n",
    "        random.seed(random_seed)\n",
    "        self.target_df = target_df.sample(frac=1).reset_index(drop= True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        traj_id = self.target_df['name'][idx]\n",
    "        target = self.target_df['target'][idx]\n",
    "        atoms = read(f'{self.root_dir}final_CO_slab/{traj_id}.traj')\n",
    "        feature_data = get_atom_property(atoms)\n",
    "\n",
    "        \n",
    "        return feature_data, target\n",
    "\n",
    "def get_train_val_test_loader(dataset, train_ratio=0.7, batch_size=50 ,valid_ratio=0.15, test_ratio=0.15, num_workers=1):\n",
    "    \n",
    "    total_size = len(dataset)\n",
    "    indices = list(range(total_size))\n",
    "    # train_ratio = 0.7\n",
    "    # valid_ratio = 0.15\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    valid_size = int(total_size * valid_ratio)\n",
    "    test_size = int(test_ratio * total_size)\n",
    "\n",
    "    train_sampler =SubsetRandomSampler(indices[:train_size])\n",
    "    valid_sampler = SubsetRandomSampler(indices[-(valid_size + test_size):-test_size])\n",
    "    test_sampler = SubsetRandomSampler(indices[-test_size:])\n",
    "    train_loader = DataLoader(dataset, batch_size = batch_size,sampler=train_sampler, num_workers= num_workers)\n",
    "    val_loader = DataLoader(dataset, batch_size = batch_size, sampler = valid_sampler, num_workers = num_workers )\n",
    "    test_loader = DataLoader(dataset, batch_size= batch_size, sampler = test_sampler, num_workers=num_workers )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--problem',type = int,  default= 100)\n",
    "    parser.add_argument('--train_step', type=int, default=200)\n",
    "    parser.add_argument('--batch_size', type=int, default=50)\n",
    "    parser.add_argument('--max_length', type=int, default=100)\n",
    "    parser.add_argument('--n_layers', type=int, default=6)\n",
    "    parser.add_argument('--hidden_size', type=int, default=512)\n",
    "    parser.add_argument('--filter_size', type=int, default=1024)\n",
    "    parser.add_argument('--warmup', type=int, default=16000)\n",
    "    parser.add_argument('--val_every', type=int, default=5)\n",
    "    parser.add_argument('--dropout', type=float, default=0.15)\n",
    "    parser.add_argument('--label_smoothing', type=float, default=0.1)\n",
    "    parser.add_argument('--model', type=str, default='Adsormer')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output')\n",
    "    parser.add_argument('--data_dir', type=str, default='./data')\n",
    "    parser.add_argument('--no_cuda', action='store_true')\n",
    "    train_group = parser.add_mutually_exclusive_group()\n",
    "    train_group.add_argument('--train-ratio', default=0.7, type=float, metavar='N',\n",
    "                    help='number of training data to be loaded (default none)')\n",
    "    train_group.add_argument('--train-size', default=None, type=int, metavar='N',\n",
    "                         help='number of training data to be loaded (default none)')\n",
    "    valid_group = parser.add_mutually_exclusive_group()\n",
    "    valid_group.add_argument('--val-ratio', default=0.15, type=float, metavar='N',\n",
    "                        help='percentage of validation data to be loaded (default '\n",
    "                             '0.1)')\n",
    "    valid_group.add_argument('--val-size', default=None, type=int, metavar='N',\n",
    "                             help='number of validation data to be loaded (default '\n",
    "                                  '1000)')\n",
    "    test_group = parser.add_mutually_exclusive_group()\n",
    "    test_group.add_argument('--test-ratio', default=0.15, type=float, metavar='N',\n",
    "                        help='percentage of test data to be loaded (default 0.1)')\n",
    "    test_group.add_argument('--test-size', default=None, type=int, metavar='N',\n",
    "                            help='number of test data to be loaded (default 1000)')\n",
    "    parser.add_argument('--optim', default='SGD', type=str, metavar='SGD',\n",
    "                    help='choose an optimizer, SGD or Adam, (default: SGD)')\n",
    "    parser.add_argument('--parallel', action='store_true')\n",
    "    # parser.add_argument('--summary_grad', action='store_true')\n",
    "    opt = parser.parse_args()\n",
    "    global_step = 0.\n",
    "    \n",
    "    dataset= make_dataset(root_dir= '/home/cut6089/research/GASpy/')\n",
    "    train_loader, val_loader, test_loader = get_train_val_test_loader(dataset, batch_size= opt.batch_size,\n",
    "                                                                        train_ratio = opt.train_ratio, valid_ratio= opt.val_ratio,\n",
    "                                                                        test_ratio = opt.test_ratio, )\n",
    "   \n",
    "    device = torch.device('cpu' if opt.no_cuda else 'cuda')\n",
    "    \n",
    "    \n",
    "    from Adsormer import Transformer\n",
    "    model = Transformer(i_vocab_size = opt.problem, t_vocab_size = 1,n_layers=opt.n_layers, hidden_size=opt.hidden_size,\n",
    "                         filter_size=opt.filter_size, dropout_rate=opt.dropout)\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    criterion = nn.MSELoss()\n",
    "    if opt.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr = 0.001,\n",
    "                              momentum= 0.9)\n",
    "    elif opt.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "    for epoch in  range(opt.train_step):\n",
    "        print(\"Epoch\", epoch)\n",
    "        train(train_loader, model,global_step=global_step, criterion=criterion, optimizer= optimizer, epoch=epoch)\n",
    "        # val_loss = validation(val_loader,model= model, global_step=global_step ,criterion=criterion)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# \tmain()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2284544-1972-4cc4-9e31-ffc92dd0a4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--problem PROBLEM]\n",
      "                             [--train_step TRAIN_STEP]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--max_length MAX_LENGTH] [--n_layers N_LAYERS]\n",
      "                             [--hidden_size HIDDEN_SIZE]\n",
      "                             [--filter_size FILTER_SIZE] [--warmup WARMUP]\n",
      "                             [--val_every VAL_EVERY] [--dropout DROPOUT]\n",
      "                             [--label_smoothing LABEL_SMOOTHING]\n",
      "                             [--model MODEL] [--output_dir OUTPUT_DIR]\n",
      "                             [--data_dir DATA_DIR] [--no_cuda]\n",
      "                             [--train-ratio N | --train-size N]\n",
      "                             [--val-ratio N | --val-size N]\n",
      "                             [--test-ratio N | --test-size N] [--optim SGD]\n",
      "                             [--parallel]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/cut6089/.local/share/jupyter/runtime/kernel-fcdf810e-4c6f-443f-a76e-307872cb6708.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7a1c7-b019-4a90-9294-20a8a3aab72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
