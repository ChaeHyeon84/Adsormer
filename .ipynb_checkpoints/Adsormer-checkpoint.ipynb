{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0cd004-03fe-44d7-bae1-9989463e8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ase.io import read,write\n",
    "from ase import Atoms, Atom\n",
    "from ase.visualize import view\n",
    "\n",
    "import numpy as np\n",
    "from pymatgen.core.structure import Structure\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "import pymatgen as mg\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchtext import data # torchtext.data 임포트\n",
    "from torchtext.data import Iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# from data import CIFData, AtomCustomJSONInitializer, GaussianDistance\n",
    "import os\n",
    "import csv\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27bb379-2bd8-40e4-9b3f-a896bd9e4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/cut6089/research/GASpy/data/co_motif.csv')\n",
    "\n",
    "target_df = df[['name','target' ]]\n",
    "# target_df.to_csv('/home/cut6089/research/GASpy/data/co_target.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8dcaa1-390f-4e5f-9a12-d3a6aec3101d",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b1632c5d-a438-4902-ad92-d097d7fadbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_adsorbates(atoms):\n",
    "    copied_atoms= atoms.copy()\n",
    "    binding_sites = []\n",
    "    adsorbates_index = []\n",
    "    \n",
    "    for atom in atoms:\n",
    "        if atom.tag == 1:\n",
    "            binding_sites.append([atom.symbol,list(atom.position)])\n",
    "            adsorbates_index.append(atom.index)\n",
    "    del copied_atoms[adsorbates_index]\n",
    "    bare_slab = copied_atoms.copy()\n",
    "    \n",
    "    return binding_sites, bare_slab\n",
    "\n",
    "def get_nearest_atoms(atoms):\n",
    "    # view(atoms)\n",
    "    binding_sites, slab = remove_adsorbates(atoms)\n",
    "    binding_sites.sort(key = lambda x: x[1][2] )    \n",
    "    \n",
    "    copied_atom = slab.copy()\n",
    "    copied_atom += Atom(binding_sites[0][0],binding_sites[0][1],tag =1 )\n",
    "    copied_atom = copied_atom.repeat((3,3,1))\n",
    "    ads_index = np.where((copied_atom.get_tags()) ==1)[0][4]\n",
    "    \n",
    "    structure = AseAtomsAdaptor.get_structure(copied_atom)\n",
    "    # nn = structure.get_neighbors(site=structure[ads_index] , r= min(structure.lattice.abc))\n",
    "    nn = structure.get_neighbors(site=structure[ads_index] , r= 10)\n",
    "    nn.sort(key = lambda x : x[1]) # sort nearest atoms\n",
    "    nn_index = [nn[i][2] for i in range(len(nn))]\n",
    "    nn_distances =[nn[i][1] for i in range(len(nn))]\n",
    "    # nn_positions = [list(copied_atom[i].position) for i in range(len(nn_index))]\n",
    "    \n",
    "    \n",
    "    # view(copied_atom)\n",
    "    return copied_atom, nn_index,nn_distances\n",
    "\n",
    "def get_atom_property(atoms):\n",
    "    global feature\n",
    "    feature_df = pd.DataFrame(columns=[f'feat{i}' for i in range(10)])\n",
    "    atom, nn_index,nn_distances= get_nearest_atoms(atoms)\n",
    "    elements = [atom[i].symbol for i in nn_index[:15]]\n",
    "    distances = nn_distances[:15]\n",
    "    def block_to_num(block):\n",
    "        \"\"\"\n",
    "        Convert blokc to number\n",
    "\n",
    "        Args:\n",
    "            block (str) : 's', 'p', 'd' or 'f'\n",
    "\n",
    "        Return:\n",
    "            int : 1, 2, 3, 4\n",
    "        \"\"\"\n",
    "\n",
    "        if block == 's':\n",
    "            return 1\n",
    "        elif block == 'p':\n",
    "            return 2\n",
    "        elif block == 'd':\n",
    "            return 3\n",
    "        elif block == 'f':\n",
    "            return 4\n",
    "\n",
    "    for el, distance in zip(elements, distances):\n",
    "        e = mg.core.Element(el)\n",
    "        atomic_number = e.Z\n",
    "        average_ionic_radius = e.average_ionic_radius.real\n",
    "\n",
    "        # Lowest oxidiation state of the element is used as common oxidation state\n",
    "        common_oxidation_states = e.common_oxidation_states[0]\n",
    "        Pauling_electronegativity = e.X\n",
    "        row = e.row\n",
    "        group = e.group\n",
    "        thermal_conductivity = e.thermal_conductivity.real\n",
    "        boiling_point = e.boiling_point.real\n",
    "        melting_point = e.melting_point.real\n",
    "        block = block_to_num(e.block)\n",
    "        IE = e.ionization_energy\n",
    "        \n",
    "        feature_df.loc[len(feature_df)] = [atomic_number, common_oxidation_states, Pauling_electronegativity, \n",
    "                                           row, group, thermal_conductivity, boiling_point,\n",
    "                                           melting_point, block, IE]\n",
    "    feature = feature_df.to_numpy()\n",
    "    feature = feature/distance\n",
    "    feature = torch.Tensor(feature)\n",
    "    \n",
    "    # nn_positions = [list(atom[i].position) for i in range(len(nn_index))]\n",
    "    # R = np.array(nn_positions)\n",
    "    # Dij = np.linalg.norm(R[:, None, :] - R[None,:,:], axis =-1)\n",
    "    # edge = torch.Tensor(Dij[:15,:15])\n",
    "    # feature.unsqueeze_(0).shape\n",
    "    return feature\n",
    "\n",
    "def get_edge_feature(atoms):\n",
    "    atom, nn_index,nn_distances = get_nearest_atoms(atoms)\n",
    "    nn_positions = [list(atom[i].position) for i in nn_index]\n",
    "    R = np.array(nn_positions)\n",
    "    Dij = np.linalg.norm(R[:, None, :] - R[None,:,:], axis =-1)\n",
    "    edge = torch.Tensor(Dij[:15,:15])\n",
    "    return edge\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f5d64a03-c665-4b48-b893-ec4bfb74a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset(Dataset):\n",
    "    def __init__(self, root_dir, dmin = 0, step = 0.2, random_seed = 123):\n",
    "        self.root_dir = root_dir\n",
    "        target_file = os.path.join(self.root_dir, 'data/co_target.csv')\n",
    "        target_df = pd.read_csv(f'{self.root_dir}data/co_target.csv')\n",
    "        random.seed(random_seed)\n",
    "        self.target_df = target_df.sample(frac=1).reset_index(drop= True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        traj_id = self.target_df['name'][idx]\n",
    "        target = self.target_df['target'][idx]\n",
    "        atoms = read(f'{self.root_dir}final_CO_slab/{traj_id}.traj')\n",
    "        feature = get_atom_property(atoms)\n",
    "        # feature= torch.Tensor(feature)\n",
    "        edge = get_edge_feature(atoms)\n",
    "        # edge = torch.Tensor(edge)\n",
    "        # target= torch.Tensor(float(target))\n",
    "        \n",
    "        return feature,edge, target\n",
    "    \n",
    "def collate_fn(dataset_list):\n",
    "    \"\"\"\n",
    "    list of tuples for each data point.\n",
    "    \"\"\"\n",
    "    batch_feature = []\n",
    "    batch_target = []\n",
    "    batch_edge = []\n",
    "    for  i, (feature,edge, target) in enumerate(dataset_list):\n",
    "        batch_feature.append(feature)\n",
    "        batch_edge.append(edge)\n",
    "        batch_target.append(target)\n",
    "    batch_feature = torch.nn.utils.rnn.pad_sequence(batch_feature, batch_first = True) \n",
    "    # batch_edge = torch.nn.utils.rnn.pad_sequence(batch_edge, batch_first = True) \n",
    "    batch_target = torch.Tensor(batch_target)\n",
    "    return batch_feature,batch_edge,batch_target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "185838a4-8cde-4e0b-9bde-3db1debb7aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mp-10010_5d83020d30582ea2977a314b</td>\n",
       "      <td>-1.756184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mp-10010_5d83cd3a4eaf9091a4055c72</td>\n",
       "      <td>-1.496603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mp-10010_5d83f778dafe868ae5d44057</td>\n",
       "      <td>-1.769610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mp-10010_5d8688095436ecdede9a240e</td>\n",
       "      <td>-0.365174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mp-10010_5d8688095436ecdede9a2467</td>\n",
       "      <td>-1.797853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18450</th>\n",
       "      <td>18450</td>\n",
       "      <td>mvc-16102_5d83f779dafe868ae5d46655</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18451</th>\n",
       "      <td>18451</td>\n",
       "      <td>mvc-16102_5d83f779dafe868ae5d46656</td>\n",
       "      <td>0.008152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18452</th>\n",
       "      <td>18452</td>\n",
       "      <td>mvc-16102_5d83f779dafe868ae5d46657</td>\n",
       "      <td>0.005785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18453</th>\n",
       "      <td>18453</td>\n",
       "      <td>mvc-16380_5d83020f30582ea2977aba24</td>\n",
       "      <td>-0.069272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18454</th>\n",
       "      <td>18454</td>\n",
       "      <td>mvc-16396_5d83021030582ea2977ad1c2</td>\n",
       "      <td>-0.396211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18455 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                name    target\n",
       "0               0   mp-10010_5d83020d30582ea2977a314b -1.756184\n",
       "1               1   mp-10010_5d83cd3a4eaf9091a4055c72 -1.496603\n",
       "2               2   mp-10010_5d83f778dafe868ae5d44057 -1.769610\n",
       "3               3   mp-10010_5d8688095436ecdede9a240e -0.365174\n",
       "4               4   mp-10010_5d8688095436ecdede9a2467 -1.797853\n",
       "...           ...                                 ...       ...\n",
       "18450       18450  mvc-16102_5d83f779dafe868ae5d46655  0.000546\n",
       "18451       18451  mvc-16102_5d83f779dafe868ae5d46656  0.008152\n",
       "18452       18452  mvc-16102_5d83f779dafe868ae5d46657  0.005785\n",
       "18453       18453  mvc-16380_5d83020f30582ea2977aba24 -0.069272\n",
       "18454       18454  mvc-16396_5d83021030582ea2977ad1c2 -0.396211\n",
       "\n",
       "[18455 rows x 3 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9bc0d068-616d-4a8c-a9af-777de3390a69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 7.8284e-02, 6.9831e-02, 3.8958e-02, 7.2993e-02, 1.8168e-02,\n",
       "         3.0975e-02, 7.8004e-02, 1.1768e-02, 1.3648e-02, 2.1054e-02, 6.3273e-03,\n",
       "         1.7054e-02, 3.1656e-03, 1.2765e-02],\n",
       "        [7.8284e-02, 1.0000e+00, 5.4920e-03, 1.5807e-02, 2.6413e-02, 2.7784e-03,\n",
       "         7.7862e-02, 3.4934e-02, 1.1076e-03, 7.4792e-02, 2.4565e-03, 1.9477e-02,\n",
       "         2.7001e-03, 1.9621e-03, 5.4577e-03],\n",
       "        [6.9831e-02, 5.4920e-03, 1.0000e+00, 1.3008e-02, 2.7520e-02, 3.1961e-02,\n",
       "         3.9369e-03, 1.9589e-02, 7.3588e-02, 1.1917e-03, 8.4791e-02, 7.5142e-04,\n",
       "         2.9844e-02, 1.3712e-03, 6.0122e-03],\n",
       "        [3.8958e-02, 1.5807e-02, 1.3008e-02, 1.0000e+00, 2.8637e-03, 8.3179e-02,\n",
       "         1.9621e-03, 1.1974e-02, 1.5240e-02, 1.2566e-02, 2.4015e-03, 3.2884e-02,\n",
       "         2.1613e-02, 7.7862e-02, 7.0387e-02],\n",
       "        [7.2993e-02, 2.6413e-02, 2.7520e-02, 2.8637e-03, 1.0000e+00, 1.9621e-03,\n",
       "         8.3179e-02, 2.8712e-02, 2.2499e-03, 2.9073e-03, 2.3907e-02, 7.1228e-04,\n",
       "         2.7443e-03, 2.3977e-04, 1.1950e-03],\n",
       "        [1.8168e-02, 2.7784e-03, 3.1961e-02, 8.3179e-02, 1.9621e-03, 1.0000e+00,\n",
       "         5.7002e-04, 3.8856e-03, 8.2130e-02, 1.2377e-03, 3.7618e-03, 2.7988e-03,\n",
       "         2.2540e-02, 2.6413e-02, 1.7543e-02],\n",
       "        [3.0975e-02, 7.7862e-02, 3.9369e-03, 1.9621e-03, 8.3179e-02, 5.7002e-04,\n",
       "         1.0000e+00, 3.0706e-02, 4.5595e-04, 1.2494e-02, 4.2164e-03, 1.7392e-03,\n",
       "         1.1451e-03, 1.8853e-04, 1.0739e-03],\n",
       "        [7.8004e-02, 3.4934e-02, 1.9589e-02, 1.1974e-02, 2.8712e-02, 3.8856e-03,\n",
       "         3.0706e-02, 1.0000e+00, 5.5551e-03, 2.1695e-02, 2.8889e-02, 4.4192e-03,\n",
       "         2.9463e-02, 1.0632e-03, 2.0008e-02],\n",
       "        [1.1768e-02, 1.1076e-03, 7.3588e-02, 1.5240e-02, 2.2499e-03, 8.2130e-02,\n",
       "         4.5595e-04, 5.5551e-03, 1.0000e+00, 4.9649e-04, 1.8713e-02, 5.8326e-04,\n",
       "         7.3817e-02, 3.2529e-03, 1.2462e-02],\n",
       "        [1.3648e-02, 7.4792e-02, 1.1917e-03, 1.2566e-02, 2.9073e-03, 1.2377e-03,\n",
       "         1.2494e-02, 2.1695e-02, 4.9649e-04, 1.0000e+00, 7.1608e-04, 7.8987e-02,\n",
       "         2.4891e-03, 2.7302e-03, 1.2063e-02],\n",
       "        [2.1054e-02, 2.4565e-03, 8.4791e-02, 2.4015e-03, 2.3907e-02, 3.7618e-03,\n",
       "         4.2164e-03, 2.8889e-02, 1.8713e-02, 7.1608e-04, 1.0000e+00, 2.3500e-04,\n",
       "         2.3702e-02, 2.2223e-04, 2.9438e-03],\n",
       "        [6.3273e-03, 1.9477e-02, 7.5142e-04, 3.2884e-02, 7.1228e-04, 2.7988e-03,\n",
       "         1.7392e-03, 4.4192e-03, 5.8326e-04, 7.8987e-02, 2.3500e-04, 1.0000e+00,\n",
       "         1.8985e-03, 2.0874e-02, 1.6795e-02],\n",
       "        [1.7054e-02, 2.7001e-03, 2.9844e-02, 2.1613e-02, 2.7443e-03, 2.2540e-02,\n",
       "         1.1451e-03, 2.9463e-02, 7.3817e-02, 2.4891e-03, 2.3702e-02, 1.8985e-03,\n",
       "         1.0000e+00, 3.4946e-03, 7.7041e-02],\n",
       "        [3.1656e-03, 1.9621e-03, 1.3712e-03, 7.7862e-02, 2.3977e-04, 2.6413e-02,\n",
       "         1.8853e-04, 1.0632e-03, 3.2529e-03, 2.7302e-03, 2.2223e-04, 2.0874e-02,\n",
       "         3.4946e-03, 1.0000e+00, 1.9182e-02],\n",
       "        [1.2765e-02, 5.4577e-03, 6.0122e-03, 7.0387e-02, 1.1950e-03, 1.7543e-02,\n",
       "         1.0739e-03, 2.0008e-02, 1.2462e-02, 1.2063e-02, 2.9438e-03, 1.6795e-02,\n",
       "         7.7041e-02, 1.9182e-02, 1.0000e+00]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge = get_edge_feature(atom)\n",
    "torch.exp(-edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ea7a7989-e253-43f1-b329-40333f1dd908",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[242], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m traj_id \u001b[38;5;241m=\u001b[39m target_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[1;32m      5\u001b[0m atom \u001b[38;5;241m=\u001b[39m read(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/cut6089/research/GASpy/final_CO_slab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraj_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.traj\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m edge \u001b[38;5;241m=\u001b[39m \u001b[43mget_edge_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43matom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(edge\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[240], line 94\u001b[0m, in \u001b[0;36mget_edge_feature\u001b[0;34m(atoms)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_edge_feature\u001b[39m(atoms):\n\u001b[0;32m---> 94\u001b[0m     atom, nn_index,nn_distances \u001b[38;5;241m=\u001b[39m \u001b[43mget_nearest_atoms\u001b[49m\u001b[43m(\u001b[49m\u001b[43matoms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     nn_positions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(atom[i]\u001b[38;5;241m.\u001b[39mposition) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nn_index]\n\u001b[1;32m     96\u001b[0m     R \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(nn_positions)\n",
      "Cell \u001b[0;32mIn[240], line 21\u001b[0m, in \u001b[0;36mget_nearest_atoms\u001b[0;34m(atoms)\u001b[0m\n\u001b[1;32m     18\u001b[0m binding_sites\u001b[38;5;241m.\u001b[39msort(key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m] )    \n\u001b[1;32m     20\u001b[0m copied_atom \u001b[38;5;241m=\u001b[39m slab\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 21\u001b[0m copied_atom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Atom(binding_sites[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],binding_sites[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m],tag \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m )\n\u001b[1;32m     22\u001b[0m copied_atom \u001b[38;5;241m=\u001b[39m copied_atom\u001b[38;5;241m.\u001b[39mrepeat((\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     23\u001b[0m ads_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((copied_atom\u001b[38;5;241m.\u001b[39mget_tags()) \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ase/atoms.py:1064\u001b[0m, in \u001b[0;36mAtoms.__iadd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iadd__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ase/atoms.py:1035\u001b[0m, in \u001b[0;36mAtoms.extend\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;124;03m\"\"\"Extend atoms object by appending atoms from *other*.\"\"\"\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Atom):\n\u001b[0;32m-> 1035\u001b[0m     other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mother\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m n1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1038\u001b[0m n2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(other)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ase/atoms.py:185\u001b[0m, in \u001b[0;36mAtoms.__init__\u001b[0;34m(self, symbols, positions, numbers, tags, momenta, masses, magmoms, charges, scaled_positions, cell, pbc, celldisp, constraint, calculator, info, velocities)\u001b[0m\n\u001b[1;32m    183\u001b[0m     pbc \u001b[38;5;241m=\u001b[39m atoms\u001b[38;5;241m.\u001b[39mget_pbc()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     constraint \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m atoms\u001b[38;5;241m.\u001b[39mconstraints]\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calculator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     calculator \u001b[38;5;241m=\u001b[39m atoms\u001b[38;5;241m.\u001b[39mcalc\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_df = pd.read_csv(f'/home/cut6089/research/GASpy/data/co_target.csv')\n",
    "# target_df['target']\n",
    "for i  in  range(len(target_df)):\n",
    "    traj_id = target_df['name'][i]\n",
    "    atom = read(f'/home/cut6089/research/GASpy/final_CO_slab/{traj_id}.traj')\n",
    "    edge = get_edge_feature(atom)\n",
    "    print(edge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2fdf363a-6380-48af-b705-264984acfd7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 10])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_files = '/home/cut6089/research/GASpy/final_CO_slab/'\n",
    "           \n",
    "atoms = read(struct_files+'mp-10010_5d83020d30582ea2977a314b.traj') \n",
    "structure = AseAtomsAdaptor.get_structure(atoms)\n",
    "# nn = structure.get_neighbors(site=structure[ads_index] , r= min(structure.lattice.abc))\n",
    "\n",
    "\n",
    "atom, nn_index,nn_distances= get_nearest_atoms(atoms)\n",
    "get_edge_feature(atoms).shape\n",
    "get_atom_property(atoms).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "1a7aac66-d667-4a2f-a63e-b9d3c48f7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= make_dataset(root_dir= '/home/cut6089/research/GASpy/')\n",
    "total_size = len(dataset)\n",
    "indices = list(range(total_size))\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.15\n",
    "train_size = int(total_size * train_ratio)\n",
    "valid_size = int(total_size * valid_ratio)\n",
    "\n",
    "train_sampler =SubsetRandomSampler(indices[:train_size])\n",
    "# valid_sampler = SubsetRandomSampler(indices[:valid_size])\n",
    "train_loader = DataLoader(dataset, batch_size = 50,sampler=train_sampler, num_workers=1 )\n",
    "dataiter = iter(train_loader)\n",
    "inputs, edge, labels= next(dataiter)\n",
    "# features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1008b17b-30de-44d6-a319-f444cef5a49f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 10])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de1a3870-6053-4be7-b111-85cbc2aa6e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = features[0]\n",
    "layernorm = nn.LayerNorm(10,eps =1e-6)\n",
    "a = layernorm(a)\n",
    "a = a.view(-1).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2936021b-0606-422f-97b3-2e1fd5878d57",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         ...,\n",
       "         [-1.7256,  0.2991,  0.1027,  ..., -0.3914, -1.0141, -1.8899],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399]],\n",
       "\n",
       "        [[ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         ...,\n",
       "         [-1.7256,  0.2991,  0.1027,  ..., -0.3914, -1.0141, -1.8899],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399]],\n",
       "\n",
       "        [[ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         ...,\n",
       "         [-1.7256,  0.2991,  0.1027,  ..., -0.3914, -1.0141, -1.8899],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         ...,\n",
       "         [-1.7256,  0.2991,  0.1027,  ..., -0.3914, -1.0141, -1.8899],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399]],\n",
       "\n",
       "        [[ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         ...,\n",
       "         [-1.7256,  0.2991,  0.1027,  ..., -0.3914, -1.0141, -1.8899],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399]],\n",
       "\n",
       "        [[ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         ...,\n",
       "         [-1.7256,  0.2991,  0.1027,  ..., -0.3914, -1.0141, -1.8899],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399],\n",
       "         [ 1.1264, -0.3888, -0.5008,  ...,  0.3766, -1.8642, -1.1399]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape\n",
    "emb = nn.Embedding(150, 512)\n",
    "emb(a).view(15,10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64af0135-3dd6-4b52-b640-a34a9749d9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.9572)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat(targets,0).shape\n",
    "torch.Tensor(targets)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca32046-b0c1-4410-bd86-c71c4cafac53",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "92376e47-8b91-4ed2-904a-1c1d414388c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Transformer(nn_nums=15, feature_nums=10, n_layers=6, hidden_size=128, filter_size=2048,  dropout_rate=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "# criterion =  nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "# for i, data in enumerate(train_loader):\n",
    "#     inputs, labels = data\n",
    "#     print(inputs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c816d06b-a4d6-43e5-84d3-cb37693af431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 1.0646\n",
      "mae error: 0.8560225965175589 \n",
      "[1,     2] loss: 1.0634\n",
      "mae error: 0.8174877051300024 \n",
      "[1,     3] loss: 0.8968\n",
      "mae error: 0.6989290761481448 \n",
      "[1,     4] loss: 0.6987\n",
      "mae error: 0.6556606451350979 \n",
      "[1,     5] loss: 1.0811\n",
      "mae error: 0.8584857368784802 \n",
      "[1,     6] loss: 0.7276\n",
      "mae error: 0.662250488857492 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m  outputs\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[265], line 241\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, inputs, edge, targets)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, edge, targets):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# input_normed = self.input_normalize(inputs.float()).long()\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     out1\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout1(enc_output)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    243\u001b[0m     out2\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout2(out1)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "Cell \u001b[0;32mIn[265], line 256\u001b[0m, in \u001b[0;36mTransformer.encode\u001b[0;34m(self, inputs, edge)\u001b[0m\n\u001b[1;32m    253\u001b[0m input_embedded \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_scale\n\u001b[1;32m    254\u001b[0m input_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_emb_dropout(input_embedded)\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[265], line 187\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, inputs, edge)\u001b[0m\n\u001b[1;32m    185\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 187\u001b[0m     encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43menc_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_norm(encoder_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[265], line 95\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, edge)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge):  \u001b[38;5;66;03m# pylint: disable=arguments-differ\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention_norm(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention_dropout(y)\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m y\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[265], line 49\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, edge)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# scaled dot product\u001b[39;00m\n\u001b[1;32m     48\u001b[0m q\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale)\n\u001b[0;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m x  \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m+\u001b[39medge\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(x, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mae_errors_all = []\n",
    "\n",
    "\n",
    "for epoch in range(10): # loop over t he dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        # mae_erros = []\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, edges, labels = data\n",
    "        a,b = inputs.shape, labels.shape\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs,edges,labels)\n",
    "        outputs =  outputs.type(torch.float32)\n",
    "        loss = criterion(outputs, labels.to(torch.float32))\n",
    "        # loss.requires_grad_(True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        print(f'[{epoch +1}, {i+1:5d}] loss: {loss:.4f}')\n",
    "        mae_error = torch.mean(abs(outputs-labels))\n",
    "        mae_errors_all.append(mae_error)\n",
    "        print(f'mae error: {mae_error} ')\n",
    "        if i % 10 == 9:\n",
    "            # print(f'[{epoch +1}, {i+1:5d}] loss: {running_loss/ 2000:.3f}')\n",
    "            # print(f'mae error: {mae_error} ')\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae927fb9-0be2-4c47-abc5-dfb55269bced",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4f3f33b1-21f1-4a81-aa74-8f6c8c6528bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(x):\n",
    "    nn.init.xavier_uniform_(x.weight)\n",
    "    if x.bias is not None:\n",
    "        nn.init.constant_(x.bias, 0)\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_rate , head_size = 8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.head_size = head_size\n",
    "        self.att_size = att_size = hidden_size // head_size\n",
    "        self.scale = att_size ** -0.5\n",
    "        \n",
    "        self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias = False)\n",
    "        self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias = False)\n",
    "        self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias = False)\n",
    "        \n",
    "        self.linear_edge = nn.Linear(1, head_size, bias = False)\n",
    "        initialize_weight(self.linear_q)\n",
    "        initialize_weight(self.linear_k)\n",
    "        initialize_weight(self.linear_v)\n",
    "        \n",
    "        self.att_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(head_size * att_size, hidden_size, bias = False)\n",
    "        initialize_weight(self.output_layer)\n",
    "        \n",
    "    def forward(self, q, k, v, edge):\n",
    "        orig_q_size = q.size()\n",
    "        \n",
    "        d_k = self.att_size\n",
    "        d_v = self.att_size\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # head_i \n",
    "        q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k)\n",
    "        k = self.linear_k(k).view(batch_size, -1 , self.head_size, d_k)\n",
    "        v = self.linear_v(v).view(batch_size, -1, self.head_size, d_v)\n",
    "        edge = self.linear_edge(edge.unsqueeze(1).transpose(1,-1)).transpose(1,3)\n",
    "        \n",
    "        q= q.transpose(1,2)\n",
    "        v = v.transpose(1, 2)\n",
    "        k = k.transpose(1, 2).transpose(2,3)\n",
    "        \n",
    "        # scaled dot product\n",
    "        q.mul_(self.scale)\n",
    "        x = torch.matmul(q,k)\n",
    "        x  = x+edge\n",
    "        x = torch.softmax(x, dim = 3)\n",
    "        x = self.att_dropout(x)\n",
    "        x= x.matmul(v)\n",
    "        \n",
    "        x= x.transpose(1,2).contiguous()\n",
    "        x = x.view(batch_size, -1, self.head_size * d_v)\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return (x)\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "        initialize_weight(self.layer1)\n",
    "        initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.BatchNorm1d(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.BatchNorm1d(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge):  # pylint: disable=arguments-differ\n",
    "        y = self.self_attention_norm(x.transpose(1,2)).transpose(1,2)\n",
    "        y = self.self_attention(y, y, y,edge)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x.transpose(1,2)).transpose(1,2)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "# class DecoderLayer(nn.Module):\n",
    "#     def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "        \n",
    "#         self.self_attention_norm = nn.LayerNorm(hidden_size, eps = 1e-6)\n",
    "#         self.self_attention = MultiHeadAttentioin(hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
    "#         self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "#         self.enc_dec_attention = MultiHeadAttentioin(hidden_size, dropout_rate)\n",
    "#         self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#         self.ffn_norm =  nn.LayerNorm(hidden_size, eps = 1e-6)\n",
    "#         self.ffn =  FeedForwardNetwork(hidden_size, filter_size,  dropout_rate)\n",
    "#         self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "#     def forward(self, x, enc_output):\n",
    "#         y = self.self_attention_norm(x)\n",
    "#         y = self.self_attention(y,y,y)\n",
    "#         y = self.self_attention_dropout(y)\n",
    "#         x = x+y\n",
    "        \n",
    "#         y = self.enc_dec_attention_norm(x)\n",
    "#         y = self.enc_dec_attention_norm(y, enc_output, enc_output)\n",
    "#         y = self.enc_dec_attention_dropout(y)\n",
    "#         x= x+y\n",
    "        \n",
    "#         y= self.ffn_norm(x)\n",
    "#         y = self.ffn(y)\n",
    "#         y = self.ffn_dropout(y)\n",
    "#         x = x+y\n",
    "#         return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.enc_dec_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y)\n",
    "        y = self.self_attention_dropout(x)\n",
    "        x = x + y\n",
    "\n",
    "        if enc_output is not None:\n",
    "            y = self.enc_dec_attention_norm(x)\n",
    "            y = self.enc_dec_attention(y, enc_output, enc_output)\n",
    "            y = self.enc_dec_attention_dropout(y)\n",
    "            x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        encoders = [EncoderLayer(hidden_size= hidden_size, filter_size= filter_size, dropout_rate = dropout_rate)\n",
    "                   for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(encoders)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.last_norm = nn.BatchNorm1d(hidden_size, eps = 1e-6)\n",
    "        \n",
    "    def forward(self, inputs,edge):\n",
    "        encoder_output = inputs\n",
    "        for enc_layer in self.layers:\n",
    "            encoder_output = enc_layer(encoder_output, edge)\n",
    "        return self.last_norm(encoder_output.transpose(1,2)).transpose(1,2)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        decoders = [DecoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                   for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(decoders)\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps = 1e-6 )\n",
    "        \n",
    "    def forward(self, targets, enc_output):\n",
    "        decoder_ouput = targets\n",
    "        for dec_layer in self.layers:\n",
    "            decoder_ouput= dec_layer(decoder_ouput, enc_output)\n",
    "        return self.last_norm(decoder_ouput)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, nn_nums, feature_nums, n_layers = 6, hidden_size = 512,\n",
    "                filter_size = 2048, dropout_rate = 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_scale=  hidden_size ** 0.5\n",
    "        \n",
    "        # self.t_vocab_embedding = nn.Embedding(t_vocab_size, hidden_size)\n",
    "        self.input_normalize = nn.BatchNorm1d(feature_nums,eps =1e-6)\n",
    "        self.edge_normalize  =   nn.BatchNorm1d(nn_nums, eps=1e-6)\n",
    "        self.target_normalize = nn.LayerNorm(1, eps= 1e-6)\n",
    "        \n",
    "        # self.t_vocab_embedding = nn.Embedding(t_vocab_size, hidden_size)\n",
    "        # nn.init.normal_(self.t_vocab_embedding.weight, mean = 0, std = hidden_size ** -0.5)\n",
    "        # self.t_emb_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.i_vocab_embedding1 = nn.Linear(feature_nums,hidden_size)\n",
    "        # self.i_vocab_embedding2 = nn.Linear(1, hidden_size)\n",
    "        self.edge_embedding = nn.Linear(nn_nums, hidden_size)\n",
    "        nn.init.normal_(self.i_vocab_embedding1.weight, mean = 0 , std = hidden_size ** -0.5)\n",
    "        self.i_emb_dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder = Encoder(hidden_size, filter_size, dropout_rate, n_layers)\n",
    "        self.decoder = Decoder(hidden_size, filter_size, dropout_rate, n_layers)\n",
    "        # self.out = nn.Linear(t_vocab_size *1, 1)\n",
    "        \n",
    "        self.out1 = nn.Linear(hidden_size, 1)\n",
    "        self.out2= nn.Linear(nn_nums,1)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, edge, targets):\n",
    "        # input_normed = self.input_normalize(inputs.float()).long()\n",
    "        batch_size = inputs.size(0)\n",
    "        enc_output = self.encode(inputs,edge)\n",
    "        out1= self.out1(enc_output).squeeze()\n",
    "        out2= self.out2(out1).squeeze()\n",
    "        return  out2\n",
    "    \n",
    "    def encode(self, inputs, edge):\n",
    "        inputs = self.input_normalize(inputs.transpose(1,2)).transpose(1,2)\n",
    "        edge = self.edge_normalize(torch.exp(edge))\n",
    "        # Input embedding\n",
    "        input_embedded = self.i_vocab_embedding1(inputs)\n",
    "        # edge_embedded = self.edge_embedding(edge)\n",
    "        # input_embedded = self.i_vocab_embedding2(input_embedded)\n",
    "        input_embedded *- self.emb_scale\n",
    "        input_embedded = self.i_emb_dropout(input_embedded)\n",
    "        \n",
    "        return self.encoder(input_embedded,edge)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     def decode(self, targets, enc_output):\n",
    "#         # target embedding\n",
    "#         targets = self.target_normalize(targets.view(-1,1).to(torch.float32)).long()\n",
    "#         target_embedded = self.t_vocab_embedding(targets.long())\n",
    "#         # target_embedded *= self.emb_scale\n",
    "#         target_embedded = self.t_emb_dropout(target_embedded).to(torch.float32)\n",
    "#         decoder_output = self.decoder(target_embedded, enc_output)\n",
    "#         # output = torch.matmul(decoder_output, self.t_vocab_embedding.weight.transpose(0,1))\n",
    "#         weights =self.t_vocab_embedding.weight.unsqueeze(0).transpose(1,2)\n",
    "#         # output = torch.matmul(decoder_output, weights)\n",
    "#         # output = self.out(output).squeeze()\n",
    "#         output = self.out(decoder_output.squeeze()).view(-1)\n",
    "\n",
    "#         return output\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ad9ffb86-b858-4983-ad75-9e52c4c559d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 8, 15, 15])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(nn_nums= 15, feature_nums=10, n_layers=6, hidden_size=512, filter_size=2048,  dropout_rate=0.1)\n",
    "features.shape\n",
    "edge.shape\n",
    "a =  nn.Linear(10, 512)(features)\n",
    "linear_q = nn.Linear(512, 8 * 64, bias = False)\n",
    "b = linear_q(a).view(50, -1, 8, 64).transpose(1, 2)\n",
    "c = b.transpose(2,3)\n",
    "mul = torch.matmul(b,c)\n",
    "edge.shape\n",
    "edge_a = nn.Linear(15,8 * 64, bias = False )(edge)\n",
    "edge_b = linear_q(edge_a).view(50, -1, 8, 64).transpose(1, 2)\n",
    "mul.shape\n",
    "edge_b.shape\n",
    "e = nn.Linear(1,8)(edge.unsqueeze(1).transpose(1,-1)).transpose(1,3)\n",
    "# e = nn\n",
    "# torch.stack([mul,e]).shape\n",
    "e[0][0].shape\n",
    "mul[0][0].shape\n",
    "(e+mul).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "39ad6cd9-942e-4952-bd41-9e378b768f41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'edge' and 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[282], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m----> 2\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'edge' and 'targets'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (15,10,512), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "53db2aed-4522-49ed-80c5-546c6511d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "519658d7-50c4-4013-a1ad-625099f85d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(nn_nums= 15, feature_nums=10, n_layers=6, hidden_size=512, filter_size=2048,  dropout_rate=0.1)\n",
    "enc_output = model.encode(inputs)\n",
    "a = nn.BatchNorm1d(15,eps =1e-6)(inputs)\n",
    "a = nn.Linear(10, 512)(a)\n",
    "a.shape\n",
    "# b= nn.Linear(1, 512)(a)\n",
    "b = nn.Linear(512,1)(enc_output).squeeze()\n",
    "nn.Linear(15,1)(b).squeeze().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f2ccac7-ea60-46d0-a5d0-554f684fa1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'i_vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_vocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_vocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      5\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'i_vocab_size'"
     ]
    }
   ],
   "source": [
    "model = Transformer(i_vocab_size= 3000, t_vocab_size=100, n_layers=6, hidden_size=512, filter_size=2048,  dropout_rate=0.1)\n",
    "\n",
    "\n",
    "hidden_size = 512\n",
    "dropout_rate = 0.1\n",
    "filter_size =2048\n",
    "head_size = 8\n",
    "att_size = hidden_size//head_size\n",
    "\n",
    "enc_output= model.encode(inputs)\n",
    "target_normalize = nn.LayerNorm(1, eps= 1e-6)\n",
    "# model.decode(inputs, labels)\n",
    "targets = target_normalize(targets.view(-1,1).float())\n",
    "t_vocab_embedding = nn.Embedding(100, 512)\n",
    "target_embedded = t_vocab_embedding(targets.long())\n",
    "target_embedded = nn.Dropout(dropout_rate)(target_embedded)\n",
    "decoder_output = Decoder(hidden_size=512, dropout_rate=0.1,filter_size = 2048, n_layers=6)(target_embedded,enc_output)\n",
    "# target_embedded.shape\n",
    "\n",
    "\n",
    "t_vocab_embedding.weight.shape\n",
    "final_output = decoder_output\n",
    "weights = torch.tensor(t_vocab_embedding.weight, device = 'cuda').unsqueeze(0).transpose(1,2)\n",
    "                                                                                \n",
    "\n",
    "output = torch.tensor(final_output, device= 'cuda')\n",
    "print('final shape:', output.shape)\n",
    "print('weight:', weights.shape)\n",
    "print('decoder: ', final_output.shape)\n",
    "matmul = torch.matmul(output, weights)\n",
    "print('malmul: ', matmul.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb97fb03-c843-4807-97d6-ffe9ee488625",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m target_normed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;241m50\u001b[39m, eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)(targets\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# out = model.encode(input_normed)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mdecode(targets\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), \u001b[43mout\u001b[49m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# a = nn.LayerNorm(512, eps=1e-6)(input_embedded)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m b \u001b[38;5;241m=\u001b[39m MultiHeadAttention(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)(input_embedded,input_embedded,input_embedded)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "# model.encode(feature)\n",
    "model = Transformer(i_vocab_size= 100, t_vocab_size=1, n_layers=6, hidden_size=512, filter_size=2048,  dropout_rate=0.1)\n",
    "input_normed = nn.LayerNorm(10,eps =1e-6)(inputs.float()).long()\n",
    "input_embedded = nn.Embedding(100, 512)(input_normed)\n",
    "target_normed = nn.LayerNorm(50, eps = 1e-6)(targets.float()).long()\n",
    "# out = model.encode(input_normed)\n",
    "model.decode(targets.reshape(-1,1), out )\n",
    "# a = nn.LayerNorm(512, eps=1e-6)(input_embedded)\n",
    "b = MultiHeadAttention(512, 0.1)(input_embedded,input_embedded,input_embedded)\n",
    "input_embedded.shape\n",
    "input_embedded.view(-1,200,512).shape\n",
    "EncoderLayer(hidden_size=512, filter_size=2048,  dropout_rate=0.1)(input_embedded.view(-1,200,512))\n",
    "Encoder(n_layers=6, hidden_size=512, filter_size=2048,  dropout_rate=0.1)(input_embedded.view(-1,200,512))\n",
    "model.encode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca739ba-be91-42e0-9f98-4fd4ca6707c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 5 4 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# \"H, W, N, M이 공백으로 구분되어 주어진다. (0 < H, W, N, M ≤ 50,000)\"\n",
    "inputs = input()\n",
    "inputs = [int(x) for x in inputs.split()]\n",
    "H = inputs[0]\n",
    "W = inputs[1]\n",
    "N = inputs[2]\n",
    "M = inputs[3]\n",
    "\n",
    "if 0<H<=50000 and 0<W<=50000 and 0<N<50000 and 0<M<50000:\n",
    "    i =1\n",
    "    h_num = 0\n",
    "    w_num = 0\n",
    "    while i <= H:\n",
    "        n = (N+1)*i - N\n",
    "        i+=1\n",
    "        if n <=H:\n",
    "            h_num +=1\n",
    "    i=1\n",
    "    while i <= W:\n",
    "        m = (M+1)*i - M\n",
    "        i+=1\n",
    "        if m <=W:\n",
    "            w_num +=1\n",
    "    \n",
    "    print(h_num*w_num)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "58bfce0a-2904-40ec-956c-35e06d62cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# pylint: disable=arguments-differ\n",
    "\n",
    "\n",
    "def initialize_weight(x):\n",
    "    nn.init.xavier_uniform_(x.weight)\n",
    "    if x.bias is not None:\n",
    "        nn.init.constant_(x.bias, 0)\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "        initialize_weight(self.layer1)\n",
    "        initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_rate, head_size=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.att_size = att_size = hidden_size // head_size\n",
    "        self.scale = att_size ** -0.5\n",
    "\n",
    "        self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        initialize_weight(self.linear_q)\n",
    "        initialize_weight(self.linear_k)\n",
    "        initialize_weight(self.linear_v)\n",
    "\n",
    "        self.att_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.output_layer = nn.Linear(head_size * att_size, hidden_size,\n",
    "                                      bias=False)\n",
    "        initialize_weight(self.output_layer)\n",
    "\n",
    "    def forward(self, q, k, v, mask, cache=None):\n",
    "        orig_q_size = q.size()\n",
    "\n",
    "        d_k = self.att_size\n",
    "        d_v = self.att_size\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
    "        q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k)\n",
    "        if cache is not None and 'encdec_k' in cache:\n",
    "            k, v = cache['encdec_k'], cache['encdec_v']\n",
    "        else:\n",
    "            k = self.linear_k(k).view(batch_size, -1, self.head_size, d_k)\n",
    "            v = self.linear_v(v).view(batch_size, -1, self.head_size, d_v)\n",
    "\n",
    "            if cache is not None:\n",
    "                cache['encdec_k'], cache['encdec_v'] = k, v\n",
    "\n",
    "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
    "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
    "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
    "\n",
    "        # Scaled Dot-Product Attention.\n",
    "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
    "        q.mul_(self.scale)\n",
    "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
    "        x.masked_fill_(mask.unsqueeze(1), -1e9)\n",
    "        x = torch.softmax(x, dim=3)\n",
    "        x = self.att_dropout(x)\n",
    "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
    "        x = x.view(batch_size, -1, self.head_size * d_v)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        assert x.size() == orig_q_size\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):  # pylint: disable=arguments-differ\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.enc_dec_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output, self_mask, i_mask, cache):\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, self_mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        if enc_output is not None:\n",
    "            y = self.enc_dec_attention_norm(x)\n",
    "            y = self.enc_dec_attention(y, enc_output, enc_output, i_mask,\n",
    "                                       cache)\n",
    "            y = self.enc_dec_attention_dropout(y)\n",
    "            x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        encoders = [EncoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(encoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        encoder_output = inputs\n",
    "        for enc_layer in self.layers:\n",
    "            encoder_output = enc_layer(encoder_output, mask)\n",
    "        return self.last_norm(encoder_output)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        decoders = [DecoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(decoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, targets, enc_output, i_mask, t_self_mask, cache):\n",
    "        decoder_output = targets\n",
    "        for i, dec_layer in enumerate(self.layers):\n",
    "            layer_cache = None\n",
    "            if cache is not None:\n",
    "                if i not in cache:\n",
    "                    cache[i] = {}\n",
    "                layer_cache = cache[i]\n",
    "            decoder_output = dec_layer(decoder_output, enc_output,\n",
    "                                       t_self_mask, i_mask, layer_cache)\n",
    "        return self.last_norm(decoder_output)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, i_vocab_size, t_vocab_size,\n",
    "                 n_layers=6,\n",
    "                 hidden_size=512,\n",
    "                 filter_size=2048,\n",
    "                 dropout_rate=0.1,\n",
    "                 share_target_embedding=True,\n",
    "                 has_inputs=True,\n",
    "                 src_pad_idx=None,\n",
    "                 trg_pad_idx=None):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_scale = hidden_size ** 0.5\n",
    "        self.has_inputs = has_inputs\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "        self.t_vocab_embedding = nn.Embedding(t_vocab_size, hidden_size)\n",
    "        nn.init.normal_(self.t_vocab_embedding.weight, mean=0,\n",
    "                        std=hidden_size**-0.5)\n",
    "        self.t_emb_dropout = nn.Dropout(dropout_rate)\n",
    "        self.decoder = Decoder(hidden_size, filter_size,\n",
    "                               dropout_rate, n_layers)\n",
    "\n",
    "        if has_inputs:\n",
    "            if not share_target_embedding:\n",
    "                self.i_vocab_embedding = nn.Embedding(i_vocab_size,\n",
    "                                                      hidden_size)\n",
    "                nn.init.normal_(self.i_vocab_embedding.weight, mean=0,\n",
    "                                std=hidden_size**-0.5)\n",
    "            else:\n",
    "                self.i_vocab_embedding = self.t_vocab_embedding\n",
    "\n",
    "            self.i_emb_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "            self.encoder = Encoder(hidden_size, filter_size,\n",
    "                                   dropout_rate, n_layers)\n",
    "\n",
    "        # For positional encoding\n",
    "        num_timescales = self.hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "        log_timescale_increment = (\n",
    "            math.log(float(max_timescale) / float(min_timescale)) /\n",
    "            max(num_timescales - 1, 1))\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        enc_output, i_mask = None, None\n",
    "        if self.has_inputs:\n",
    "            i_mask = utils.create_pad_mask(inputs, self.src_pad_idx)\n",
    "            enc_output = self.encode(inputs, i_mask)\n",
    "\n",
    "        t_mask = utils.create_pad_mask(targets, self.trg_pad_idx)\n",
    "        target_size = targets.size()[1]\n",
    "        t_self_mask = utils.create_trg_self_mask(target_size,\n",
    "                                                 device=targets.device)\n",
    "        return self.decode(targets, enc_output, i_mask, t_self_mask, t_mask)\n",
    "\n",
    "    def encode(self, inputs, i_mask):\n",
    "        # Input embedding\n",
    "        input_embedded = self.i_vocab_embedding(inputs)\n",
    "        input_embedded.masked_fill_(i_mask.squeeze(1).unsqueeze(-1), 0)\n",
    "        input_embedded *= self.emb_scale\n",
    "        input_embedded += self.get_position_encoding(inputs)\n",
    "        input_embedded = self.i_emb_dropout(input_embedded)\n",
    "\n",
    "        return self.encoder(input_embedded, i_mask)\n",
    "\n",
    "    def decode(self, targets, enc_output, i_mask, t_self_mask, t_mask,\n",
    "               cache=None):\n",
    "        # target embedding\n",
    "        target_embedded = self.t_vocab_embedding(targets)\n",
    "        target_embedded.masked_fill_(t_mask.squeeze(1).unsqueeze(-1), 0)\n",
    "\n",
    "        # Shifting\n",
    "        target_embedded = target_embedded[:, :-1]\n",
    "        target_embedded = F.pad(target_embedded, (0, 0, 1, 0))\n",
    "\n",
    "        target_embedded *= self.emb_scale\n",
    "        target_embedded += self.get_position_encoding(targets)\n",
    "        target_embedded = self.t_emb_dropout(target_embedded)\n",
    "\n",
    "        # decoder\n",
    "        decoder_output = self.decoder(target_embedded, enc_output, i_mask,\n",
    "                                      t_self_mask, cache)\n",
    "        # linear\n",
    "        output = torch.matmul(decoder_output,\n",
    "                              self.t_vocab_embedding.weight.transpose(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32,\n",
    "                                device=x.device)\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)],\n",
    "                           dim=1)\n",
    "        signal = F.pad(signal, (0, 0, 0, self.hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.hidden_size)\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2eed9ff7-afb5-4f86-a288-114080931aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44154880"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(i_vocab_size= 3000, t_vocab_size=100, n_layers=6, hidden_size=512, filter_size=2048,  dropout_rate=0.1)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0c70e-b554-46aa-9c4e-9cb48f9c9d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
